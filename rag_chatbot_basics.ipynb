{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "1. **Wie funktioniert ein LLM?**  \n",
    "   - Grundlagen der Transformer-Architektur  \n",
    "   - Limitierungen von LLMs  \n",
    "\n",
    "2. **Warum braucht es RAG?**  \n",
    "   - Statisches Wissen vs. dynamische Daten  \n",
    "   - Reduktion von Halluzinationen  \n",
    "\n",
    "3. **Phasen der Dateiverarbeitung für RAG**  \n",
    "   - Embedding, Snippets, Vektordatenbank  \n",
    "\n",
    "4. **RAG vs. Finetuning**  \n",
    "   - Vor- und Nachteile im Vergleich  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Wie funktioniert ein LLM?\n",
    "\n",
    "LLMs sind komplexe neuronale Netzwerke, die auf großen Textkorpora trainiert werden.\n",
    "Sie können eine Vielzahl von Aufgaben ausführen, wie z. B. Textgenerierung, Textklassifizierung und Fragebeantwortung.\n",
    "\n",
    "LLMs verwenden Selbstaufmerksamkeitsmechanismen (`self-attention`), um die Bedeutung von Wörtern in einem Satz zu verstehen \n",
    "und einen kohärenten Text zu generieren. \n",
    "\n",
    "Sie sind in der Lage, Kontext und Semantik zu erfassen und so menschliche Sprache zu imitieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Transformer-Architektur**:  \n",
    "  - Nutzt **Self-Attention-Mechanismen**, um Kontext in Texten zu erfassen.  \n",
    "  - Verarbeitet Sequenzen parallel (nicht sequenziell wie RNNs).  \n",
    "\n",
    "- **Autocomplete auf Steroiden**:  \n",
    "  - LLMs generieren Text durch Vorhersage des nächsten Tokens basierend auf Trainingsdaten.  \n",
    "  - Beispiel: Bei der Eingabe *\"Erneuerbare Energie...\"* ergänzt das Modell *\"...ist nachhaltig\"*.  \n",
    "\n",
    "- **Limitierungen**:  \n",
    "  - **Statisches Wissen**: Trainingsdaten sind \"eingefroren\" (z. B. GPT-3: Daten bis 2021).  \n",
    "  - **Kontextfenster**: Begrenzte Token-Kapazität (z. B. 4k–128k Tokens).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Warum braucht es RAG?\n",
    "- **Problem**: LLMs können keine **aktuellen oder domänenspezifischen Daten** abrufen.  \n",
    "  - Beispiel: *\"Wie war das Wetter gestern in Berlin?\"* → LLM ohne RAG halluziniert Antworten.  \n",
    "\n",
    "- **Lösung mit RAG**:  \n",
    "  - **Retrieval**: Externe Datenbanken (z. B. PDFs, Webseiten) durchsuchen.  \n",
    "  - **Augmented Generation**: Gefundene Kontexte in den Prompt integrieren.  \n",
    "\n",
    "- **Vorteile**:  \n",
    "  - **Aktualität**: Antworten basieren auf Echtzeitdaten.  \n",
    "  - **Transparenz**: Quellen der Information sind nachvollziehbar.\n",
    "\n",
    "RAG löst diese Probleme, indem es externe Datenquellen in den Generierungsprozess einbindet. \n",
    "Es ermöglicht dem Chatbot, auf aktuelle und relevante Informationen zuzugreifen und \n",
    "∏so genauere Antworten zu liefern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Phasen der Dateiverarbeitung für RAG\n",
    "\n",
    "1. **Embedding**: Die zu verarbeitenden Dateien werden in Embedding-Vektoren umgewandelt. Diese Vektoren sind numerische Darstellungen, die die Bedeutung der Dateien erfassen.\n",
    "2. **Snippets**: Die Dateien werden in kleinere Abschnitte, sogenannte Snippets, zerlegt. Dies erleichtert die Suche und den Abruf relevanter Informationen.\n",
    "3. **Speicherung in Vektordatenbank**: Die Embedding-Vektoren und Snippets werden in einer Vektordatenbank gespeichert. Diese Datenbank ermöglicht einen effizienten Abruf der Informationen, wenn der Chatbot eine Anfrage erhält."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Embedding-Vektoren**\n",
    "Embedding-Vektoren sind mathematische Darstellungen, die die Semantik von Texten erfassen. \n",
    "Sie werden durch Trainieren eines Embedding-Modells auf großen Textkorpora erstellt. \n",
    "Diese Vektoren ermöglichen es, die Ähnlichkeit zwischen Texten zu messen und so relevante Informationen abzurufen.\n",
    "\n",
    "#### **Snippets**\n",
    "Snippets sind kleine Textabschnitte, die aus den ursprünglichen Dateien extrahiert werden. \n",
    "Sie stellen eine Zusammenfassung der wichtigsten Informationen dar und erleichtern die Suche nach spezifischen Inhalten.\n",
    "\n",
    "#### **Vektordatenbank**\n",
    "Eine Vektordatenbank speichert Embedding-Vektoren und ermöglicht es, ähnliche Vektoren effizient abzurufen. \n",
    "Sie ist ein entscheidender Bestandteil des RAG-Prozesses, da sie den schnellen Abruf relevanter Informationen ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 1: **Indexing**  / **Creating Chunks**\n",
    "1. **Laden**: PDFs, CSVs oder Webseiteninhalte extrahieren (z. B. mit `PyPDFLoader`).  \n",
    "2. **Teilen**: Dokumente in Chunks zerlegen (z. B. `RecursiveCharacterTextSplitter`).  \n",
    "3. **Embedding**: Text mit Modellen wie BERT oder OpenAI in Vektoren umwandeln.  \n",
    "4. **Speichern**: Vektoren in Datenbanken wie FAISS oder Qdrant ablegen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/_images/rag-high-level-architecture.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/_images/embedding-similarity.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 2: **Retrieval & Generierung**  \n",
    "5. **Ähnlichkeitssuche**: Benutzerfragen werden in Vektoren umgewandelt und mit der Datenbank abgeglichen.  \n",
    "6. **Antwortgenerierung**: LLM kombiniert Top-Kontexte mit der Frage für präzise Antworten. \n",
    "\n",
    "```python\n",
    "# Beispiel: Text-Splitting mit LangChain\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. RAG vs. Finetuning\n",
    "Kriterium | RAG | Finetuning |\n",
    "--- | --- | --- |\n",
    "Datenaktualität | Dynamisch (Echtzeit-Datenbanken) | Statisch (Trainingsdatensatz) |\n",
    "Kosten | Gering (kein erneutes Training) | Hoch (GPU-Ressourcen) |\n",
    "Anwendungsfall | Breite Domänen + aktuelle Infos | Spezialisierte Tasks (z. B. Medizin) |\n",
    "Nachteile | Latenz durch Retrieval | Overfitting, Catastrophic Forgetting |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Empfehlungen:\n",
    "\n",
    "  - **RAG:** Für Chatbots mit Webseiten/PDF-Anbindung oder FAQs\n",
    "  - **Finetuning:** Für niche Domänen mit stabilen Daten (z. B. juristische Sprache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zusammenfassung\n",
    "RAG und Finetuning sind unterschiedliche Ansätze, um die Leistung von LLMs zu verbessern. RAG konzentriert sich auf die Integration externer Datenquellen, während Finetuning das Modell auf eine bestimmte Aufgabe hin optimiert.\n",
    "\n",
    "  - LLMs sind mächtig, aber durch statisches Wissen begrenzt.\n",
    "  - RAG ergänzt LLMs mit dynamischen Daten für präzisere Antworten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Langchain\n",
    "Langchain ist ein mächtiges Framework, das die Entwicklung von RAG-basierten Anwendungen vereinfacht und beschleunigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**1. Modularität und Flexibilität:** \n",
    "\n",
    "Langchain bietet eine modulare Architektur, die es Entwicklern ermöglicht, verschiedene Komponenten des RAG-Prozesses einfach zu integrieren und anzupassen. Sie können verschiedene LLM-Modelle, Embedding-Modelle und Vektordatenbanken auswählen und kombinieren, um die beste Leistung für Ihre spezifischen Anforderungen zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2. Effiziente Datenverarbeitung:** \n",
    "\n",
    "Langchain bietet eine Reihe von Tools und Klassen, die die Verarbeitung und Vorbereitung von Daten für den RAG-Prozess vereinfachen. Es unterstützt die Extraktion von Text aus verschiedenen Quellen, die Tokenisierung und das Erstellen von Embedding-Vektoren. Durch die Nutzung von Langchain können Sie Zeit und Aufwand sparen, da viele dieser Aufgaben bereits implementiert und optimiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**3. Skalierbarkeit und Leistung:**\n",
    "\n",
    "Langchain ist darauf ausgelegt, mit großen Datenmengen und komplexen Abfragen umzugehen. Es nutzt effiziente Algorithmen und Datenstrukturen, um den Abruf relevanter Informationen zu beschleunigen. Durch die Einbindung von Langchain können Sie die Leistung Ihres Chatbots verbessern und eine schnellere Antwortzeit erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**4. Integration mit anderen Tools:**\n",
    "\n",
    "Langchain bietet eine breite Palette an Integrationen mit anderen beliebten Tools und Bibliotheken. Sie können Langchain mit Streamlit, einem beliebten Framework für die Erstellung interaktiver Web-Anwendungen, kombinieren, um eine benutzerfreundliche Chatbot-Schnittstelle zu erstellen. Langchain unterstützt auch die Integration mit verschiedenen LLM-Anbietern, wie z. B. OpenAI, Cohere und Hugging Face, was Ihnen die Flexibilität gibt, verschiedene Modelle zu nutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
